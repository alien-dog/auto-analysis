{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alien-dog/auto-analysis/blob/main/test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veTtOb2iXMSB",
        "outputId": "46103c5c-35da-4cdf-b324-d2e1984af41c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiB5_FJAYjIs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "feature = pd.read_csv('feature.csv', header=None)\n",
        "feature.fillna(0, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X09-PaU6aGup"
      },
      "outputs": [],
      "source": [
        "max_col = feature.abs().idxmax(axis = 1)\n",
        "feature_data = [feature.iloc[index : index+1, :].values[0][max_colum - 50: max_colum + 50] for index, max_colum in zip(feature.index, max_col)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Al1A1uTaJC9"
      },
      "outputs": [],
      "source": [
        "label_csv = pd.read_csv('label_bianhao.csv', header=None)\n",
        "label_csv.drop(label_csv.columns[0], axis=1, inplace=True)\n",
        "label_csv.fillna(0, inplace=True)\n",
        "label_data = [label_csv.iloc[index : index+1, :].values[0][max_colum - 50: max_colum + 50] for index, max_colum in zip(label_csv.index, max_col)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iUbb2-OYJ2U"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset as tDataset\n",
        "\n",
        "class DateDataNew(tDataset):\n",
        "    def __init__(self):\n",
        "        self.x,self.y = np.array(feature_data), np.array(label_data)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.x[index],self.y[index], len(self.y[index])-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UecX3YRuLY_Q"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "dataset = DateDataNew()\n",
        "\n",
        "# Assuming you have a dataset named 'dataset' and DataLoader parameters are defined\n",
        "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Define the sizes for train and test sets\n",
        "train_size = int(0.8 * len(dataset))  # 80% of the data for training\n",
        "test_size = len(dataset) - train_size  # Remaining 20% for testing\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Create DataLoader instances for train and test sets\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51y9JmCmW8z8"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.nn import MSELoss\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.nn.functional import cross_entropy, softmax\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset as tDataset\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, units, max_pred_len, start_token, end_token):\n",
        "        super().__init__()\n",
        "        self.units = units\n",
        "        # encoder\n",
        "        self.encoder = nn.LSTM(1, units, 2, batch_first=True)\n",
        "\n",
        "        # decoder\n",
        "        self.decoder_cell = nn.LSTMCell(1, units)\n",
        "        self.decoder_dense = nn.Linear(units, 1)\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.opt = torch.optim.Adam(self.parameters(), lr=0.1)\n",
        "        self.max_pred_len = max_pred_len\n",
        "        self.start_token = start_token\n",
        "        self.end_token = end_token\n",
        "\n",
        "    def encode(self, x):\n",
        "        hidden = (torch.zeros(2, x.shape[0], self.units).to(device, dtype=torch.float32), torch.zeros(2, x.shape[0], self.units).to(device, dtype=torch.float32))\n",
        "        o, (h, c) = self.encoder(x.unsqueeze(-1), hidden)\n",
        "        return h, c\n",
        "\n",
        "    def inference(self, x):\n",
        "        self.eval()\n",
        "        hx, cx = self.encode(x)\n",
        "        hx, cx = hx[0], cx[0]\n",
        "        start = torch.tensor(np.array([self.start_token])).to(device, dtype=torch.float32)\n",
        "        # start = start.type(torch.float32)\n",
        "        dec_emb_in = start.unsqueeze(-1).unsqueeze(-1)\n",
        "        dec_emb_in = dec_emb_in.permute(1, 0, 2)\n",
        "        dec_in = dec_emb_in[0]\n",
        "        output = []\n",
        "        for i in range(self.max_pred_len):\n",
        "            hx, cx = self.decoder_cell(dec_in, (hx, cx))\n",
        "            o = self.decoder_dense(hx)\n",
        "            dec_in = o.unsqueeze(-1).permute(1, 0, 2)[0]\n",
        "            output.append(o)\n",
        "        output = torch.stack(output, dim=0)\n",
        "        self.train()\n",
        "        return output.permute(1, 0, 2).view(-1, self.max_pred_len)\n",
        "\n",
        "    def train_logit(self, x, y):\n",
        "        hx, cx = self.encode(x)\n",
        "        hx, cx = hx[0], cx[0]\n",
        "        dec_in = y.unsqueeze(-1)\n",
        "        dec_emb_in = dec_in.permute(1, 0, 2)\n",
        "        output = []\n",
        "        for i in range(dec_emb_in.shape[0]):\n",
        "            hx, cx = self.decoder_cell(dec_emb_in[i], (hx, cx))\n",
        "            o = self.decoder_dense(hx)\n",
        "            output.append(o)\n",
        "        output = torch.stack(output, dim=0)\n",
        "        return output.permute(1, 0, 2)\n",
        "\n",
        "    def step(self, x, y):\n",
        "        self.opt.zero_grad()\n",
        "        batch_size = x.shape[0]\n",
        "        logit = self.train_logit(x, y)\n",
        "        dec_out = y\n",
        "        loss = self.criterion(logit.reshape(-1, 1).reshape(-1), dec_out.reshape(-1))\n",
        "        loss.backward()\n",
        "        self.opt.step()\n",
        "        return loss.detach()\n",
        "\n",
        "# class DateDataNew(tDataset):\n",
        "#     def __init__(self):\n",
        "#         feature = pd.read_csv('feature.csv', header=None)\n",
        "#         feature.fillna(0, inplace=True)\n",
        "#         feature_new = feature.iloc[:, :100]\n",
        "#         label = pd.read_csv('label_bianhao.csv', header=None)\n",
        "#         label.drop(label.columns[0], axis=1, inplace=True)\n",
        "#         label.fillna(0, inplace=True)\n",
        "#         lable_test_data = label.iloc[:, :100]\n",
        "#         self.x,self.y = np.array(feature_new.values), np.array(lable_test_data.values)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.x)\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         return self.x[index],self.y[index], len(self.y[index])-1\n",
        "\n",
        "\n",
        "def train(loopsize):\n",
        "\n",
        "    # loader = DataLoader(dataset,batch_size=32,shuffle=True)\n",
        "    model = Seq2Seq(units=64, max_pred_len=100, start_token=0.00, end_token=0.000000000000000000000001).to(device)\n",
        "    for i in range(loopsize):\n",
        "        for batch_idx, batch in enumerate(train_loader):\n",
        "            x, y, decoder_len = batch\n",
        "            x = x.to(device, dtype=torch.float32)\n",
        "            y = y.to(device, dtype=torch.float32)\n",
        "            loss = model.step(x, y)\n",
        "            if batch_idx % 70 == 0:\n",
        "                pred = model.inference(x[0:1])\n",
        "                res = pred[0]\n",
        "                src = x[0:1]\n",
        "                print(\n",
        "                    \"Epoch: \", i,\n",
        "                    \"| t: \", batch_idx,\n",
        "                    \"| loss: %.20f\" % loss,\n",
        "                )\n",
        "    torch.save(model.state_dict(), 'test_model.pth')\n",
        "        # for batch_idx, batch in enumerate(test_loader):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoYeRlVlZ1Zi",
        "outputId": "7a31633f-e4b1-4e9b-edb9-9fb96997899e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:  0 | t:  0 | loss: 0.03047632053494453430\n",
            "Epoch:  1 | t:  0 | loss: 0.04477788135409355164\n",
            "Epoch:  2 | t:  0 | loss: 0.00116467010229825974\n",
            "Epoch:  3 | t:  0 | loss: 0.00006961462349863723\n",
            "Epoch:  4 | t:  0 | loss: 0.00018962159811053425\n",
            "Epoch:  5 | t:  0 | loss: 0.00003105446376139298\n",
            "Epoch:  6 | t:  0 | loss: 0.00010958650091197342\n",
            "Epoch:  7 | t:  0 | loss: 0.00007063688099151477\n",
            "Epoch:  8 | t:  0 | loss: 0.00111447530798614025\n",
            "Epoch:  9 | t:  0 | loss: 0.00010145053238375112\n",
            "Epoch:  10 | t:  0 | loss: 0.00007121851376723498\n",
            "Epoch:  11 | t:  0 | loss: 0.00016597687499597669\n",
            "Epoch:  12 | t:  0 | loss: 0.00003645619653980248\n",
            "Epoch:  13 | t:  0 | loss: 0.00076611549593508244\n",
            "Epoch:  14 | t:  0 | loss: 0.00033004183205775917\n",
            "Epoch:  15 | t:  0 | loss: 0.00005567993866861798\n",
            "Epoch:  16 | t:  0 | loss: 0.00001096170490200166\n",
            "Epoch:  17 | t:  0 | loss: 0.00001021234311338048\n",
            "Epoch:  18 | t:  0 | loss: 0.00035013581509701908\n",
            "Epoch:  19 | t:  0 | loss: 0.00007768194336676970\n",
            "Epoch:  20 | t:  0 | loss: 0.00070929026696830988\n",
            "Epoch:  21 | t:  0 | loss: 0.00002488363315933384\n",
            "Epoch:  22 | t:  0 | loss: 0.00021924769680481404\n",
            "Epoch:  23 | t:  0 | loss: 0.00118894944898784161\n",
            "Epoch:  24 | t:  0 | loss: 0.00046158873010426760\n",
            "Epoch:  25 | t:  0 | loss: 0.00840510521084070206\n",
            "Epoch:  26 | t:  0 | loss: 0.00009151049744104967\n",
            "Epoch:  27 | t:  0 | loss: 0.00017332372954115272\n",
            "Epoch:  28 | t:  0 | loss: 0.00011970645573455840\n",
            "Epoch:  29 | t:  0 | loss: 0.00011305022781016305\n",
            "Epoch:  30 | t:  0 | loss: 0.00003112872218480334\n",
            "Epoch:  31 | t:  0 | loss: 0.00009556931763654575\n",
            "Epoch:  32 | t:  0 | loss: 0.00001874699046311434\n",
            "Epoch:  33 | t:  0 | loss: 0.00009082086035050452\n",
            "Epoch:  34 | t:  0 | loss: 0.00005870513996342197\n",
            "Epoch:  35 | t:  0 | loss: 0.00087783345952630043\n",
            "Epoch:  36 | t:  0 | loss: 0.00007238143734866753\n",
            "Epoch:  37 | t:  0 | loss: 0.00046245503472164273\n",
            "Epoch:  38 | t:  0 | loss: 0.00004545772389974445\n",
            "Epoch:  39 | t:  0 | loss: 0.00007567102875327691\n",
            "Epoch:  40 | t:  0 | loss: 0.00092064717318862677\n",
            "Epoch:  41 | t:  0 | loss: 0.00052878999849781394\n",
            "Epoch:  42 | t:  0 | loss: 0.00013700645649805665\n",
            "Epoch:  43 | t:  0 | loss: 0.00088304863311350346\n",
            "Epoch:  44 | t:  0 | loss: 0.00024275410396512598\n",
            "Epoch:  45 | t:  0 | loss: 0.00029993872158229351\n",
            "Epoch:  46 | t:  0 | loss: 0.00010000543989008293\n",
            "Epoch:  47 | t:  0 | loss: 0.00193089037202298641\n",
            "Epoch:  48 | t:  0 | loss: 0.00023031263845041394\n",
            "Epoch:  49 | t:  0 | loss: 0.00199307268485426903\n",
            "Epoch:  50 | t:  0 | loss: 0.00006351203046506271\n",
            "Epoch:  51 | t:  0 | loss: 0.00081164133735001087\n",
            "Epoch:  52 | t:  0 | loss: 0.00001738196078804322\n",
            "Epoch:  53 | t:  0 | loss: 0.00021001711138524115\n",
            "Epoch:  54 | t:  0 | loss: 0.00054730509873479605\n",
            "Epoch:  55 | t:  0 | loss: 0.00002832446079992224\n",
            "Epoch:  56 | t:  0 | loss: 0.00004353108306531794\n",
            "Epoch:  57 | t:  0 | loss: 0.00007947614358272403\n",
            "Epoch:  58 | t:  0 | loss: 0.00070467224577441812\n",
            "Epoch:  59 | t:  0 | loss: 0.00038898256025277078\n",
            "Epoch:  60 | t:  0 | loss: 0.00066720857284963131\n",
            "Epoch:  61 | t:  0 | loss: 0.00049899844452738762\n",
            "Epoch:  62 | t:  0 | loss: 0.00075529911555349827\n",
            "Epoch:  63 | t:  0 | loss: 0.00040183239616453648\n",
            "Epoch:  64 | t:  0 | loss: 0.00767341861501336098\n",
            "Epoch:  65 | t:  0 | loss: 0.00205574836581945419\n",
            "Epoch:  66 | t:  0 | loss: 0.00025692617055028677\n",
            "Epoch:  67 | t:  0 | loss: 0.00071463303174823523\n",
            "Epoch:  68 | t:  0 | loss: 0.00027487473562359810\n",
            "Epoch:  69 | t:  0 | loss: 0.00215999130159616470\n",
            "Epoch:  70 | t:  0 | loss: 0.00031082684290595353\n",
            "Epoch:  71 | t:  0 | loss: 0.00010737378761405125\n",
            "Epoch:  72 | t:  0 | loss: 0.00402897549793124199\n",
            "Epoch:  73 | t:  0 | loss: 0.00087692477973178029\n",
            "Epoch:  74 | t:  0 | loss: 0.00026810448616743088\n",
            "Epoch:  75 | t:  0 | loss: 0.00017884567205328494\n",
            "Epoch:  76 | t:  0 | loss: 0.00309883384034037590\n",
            "Epoch:  77 | t:  0 | loss: 0.00005726000381400809\n",
            "Epoch:  78 | t:  0 | loss: 0.00857094489037990570\n",
            "Epoch:  79 | t:  0 | loss: 0.00004964257095707580\n",
            "Epoch:  80 | t:  0 | loss: 0.00001803589839255437\n",
            "Epoch:  81 | t:  0 | loss: 0.00006752795889042318\n",
            "Epoch:  82 | t:  0 | loss: 0.00009742671682033688\n",
            "Epoch:  83 | t:  0 | loss: 0.00001294149114983156\n",
            "Epoch:  84 | t:  0 | loss: 0.00004788110527442768\n",
            "Epoch:  85 | t:  0 | loss: 0.00012958497973158956\n",
            "Epoch:  86 | t:  0 | loss: 0.02116272225975990295\n",
            "Epoch:  87 | t:  0 | loss: 0.13835720717906951904\n",
            "Epoch:  88 | t:  0 | loss: 0.00099526357371360064\n",
            "Epoch:  89 | t:  0 | loss: 0.00006603857036679983\n",
            "Epoch:  90 | t:  0 | loss: 0.00002227090044470970\n",
            "Epoch:  91 | t:  0 | loss: 0.00805454701185226440\n",
            "Epoch:  92 | t:  0 | loss: 0.00003666952761705033\n",
            "Epoch:  93 | t:  0 | loss: 0.00014614699466619641\n",
            "Epoch:  94 | t:  0 | loss: 0.00007087057747412473\n",
            "Epoch:  95 | t:  0 | loss: 0.00000790235571912490\n",
            "Epoch:  96 | t:  0 | loss: 0.00006799284892622381\n",
            "Epoch:  97 | t:  0 | loss: 0.00003428485069889575\n",
            "Epoch:  98 | t:  0 | loss: 0.00001528640132164583\n",
            "Epoch:  99 | t:  0 | loss: 0.00000634829848422669\n",
            "Epoch:  100 | t:  0 | loss: 0.00002078673969663214\n",
            "Epoch:  101 | t:  0 | loss: 0.00002600273546704557\n",
            "Epoch:  102 | t:  0 | loss: 0.00001285773942072410\n",
            "Epoch:  103 | t:  0 | loss: 0.00000641938868284342\n",
            "Epoch:  104 | t:  0 | loss: 0.00000605085870120092\n",
            "Epoch:  105 | t:  0 | loss: 0.00000662848515275982\n",
            "Epoch:  106 | t:  0 | loss: 0.00000302112084682449\n",
            "Epoch:  107 | t:  0 | loss: 0.00001496015920565696\n",
            "Epoch:  108 | t:  0 | loss: 0.00097969826310873032\n",
            "Epoch:  109 | t:  0 | loss: 0.00042104747262783349\n",
            "Epoch:  110 | t:  0 | loss: 0.00164615188259631395\n",
            "Epoch:  111 | t:  0 | loss: 0.00054071319755166769\n",
            "Epoch:  112 | t:  0 | loss: 0.00123753084335476160\n",
            "Epoch:  113 | t:  0 | loss: 0.00025108200497925282\n",
            "Epoch:  114 | t:  0 | loss: 0.00018055610416922718\n",
            "Epoch:  115 | t:  0 | loss: 0.00110757071524858475\n",
            "Epoch:  116 | t:  0 | loss: 0.00032667964114807546\n",
            "Epoch:  117 | t:  0 | loss: 0.00249184947460889816\n",
            "Epoch:  118 | t:  0 | loss: 0.00006203848897712305\n",
            "Epoch:  119 | t:  0 | loss: 0.00044344089110381901\n",
            "Epoch:  120 | t:  0 | loss: 0.00000679381082591135\n",
            "Epoch:  121 | t:  0 | loss: 0.00003348198879393749\n",
            "Epoch:  122 | t:  0 | loss: 0.00011743461072910577\n",
            "Epoch:  123 | t:  0 | loss: 0.00017934653442353010\n",
            "Epoch:  124 | t:  0 | loss: 0.00000508006542077055\n",
            "Epoch:  125 | t:  0 | loss: 0.00002948515066236723\n",
            "Epoch:  126 | t:  0 | loss: 0.00005917482121731155\n",
            "Epoch:  127 | t:  0 | loss: 0.00001063519357558107\n",
            "Epoch:  128 | t:  0 | loss: 0.00000405180617235601\n",
            "Epoch:  129 | t:  0 | loss: 0.00090272130910307169\n",
            "Epoch:  130 | t:  0 | loss: 0.00104084261693060398\n",
            "Epoch:  131 | t:  0 | loss: 0.00006879488500999287\n",
            "Epoch:  132 | t:  0 | loss: 0.00013869832037016749\n",
            "Epoch:  133 | t:  0 | loss: 0.00019552226876839995\n",
            "Epoch:  134 | t:  0 | loss: 0.00010124016262125224\n",
            "Epoch:  135 | t:  0 | loss: 0.00032606784952804446\n",
            "Epoch:  136 | t:  0 | loss: 0.00006006546755088493\n",
            "Epoch:  137 | t:  0 | loss: 0.00017500431567896158\n",
            "Epoch:  138 | t:  0 | loss: 0.00000628555199000402\n",
            "Epoch:  139 | t:  0 | loss: 0.00003001375807798468\n",
            "Epoch:  140 | t:  0 | loss: 0.00002022810076596215\n",
            "Epoch:  141 | t:  0 | loss: 0.00000234965182244196\n",
            "Epoch:  142 | t:  0 | loss: 0.00003302156619611196\n",
            "Epoch:  143 | t:  0 | loss: 0.00048644584603607655\n",
            "Epoch:  144 | t:  0 | loss: 0.00008211759268306196\n",
            "Epoch:  145 | t:  0 | loss: 0.00021619196922983974\n",
            "Epoch:  146 | t:  0 | loss: 0.00044868735130876303\n",
            "Epoch:  147 | t:  0 | loss: 0.00088341772789135575\n",
            "Epoch:  148 | t:  0 | loss: 0.00133288744837045670\n",
            "Epoch:  149 | t:  0 | loss: 0.00012645142851397395\n",
            "Epoch:  150 | t:  0 | loss: 0.00005108944969833829\n",
            "Epoch:  151 | t:  0 | loss: 0.00150422065053135157\n",
            "Epoch:  152 | t:  0 | loss: 0.00044363192864693701\n",
            "Epoch:  153 | t:  0 | loss: 0.00299466820433735847\n",
            "Epoch:  154 | t:  0 | loss: 0.00097870873287320137\n",
            "Epoch:  155 | t:  0 | loss: 0.00093616300728172064\n",
            "Epoch:  156 | t:  0 | loss: 0.00047694009845145047\n",
            "Epoch:  157 | t:  0 | loss: 0.00034818190033547580\n",
            "Epoch:  158 | t:  0 | loss: 0.00025774136884137988\n",
            "Epoch:  159 | t:  0 | loss: 0.00003583549914765172\n",
            "Epoch:  160 | t:  0 | loss: 0.00004233469371683896\n",
            "Epoch:  161 | t:  0 | loss: 0.00001188107762573054\n",
            "Epoch:  162 | t:  0 | loss: 0.00010847536759683862\n",
            "Epoch:  163 | t:  0 | loss: 0.00003391041536815464\n",
            "Epoch:  164 | t:  0 | loss: 0.00001534098919364624\n",
            "Epoch:  165 | t:  0 | loss: 0.00006192496221046895\n",
            "Epoch:  166 | t:  0 | loss: 0.00078277266584336758\n",
            "Epoch:  167 | t:  0 | loss: 0.01127458456903696060\n",
            "Epoch:  168 | t:  0 | loss: 0.00030058427364565432\n",
            "Epoch:  169 | t:  0 | loss: 0.00049690814921632409\n",
            "Epoch:  170 | t:  0 | loss: 0.00151988968718796968\n",
            "Epoch:  171 | t:  0 | loss: 0.00017581188876647502\n",
            "Epoch:  172 | t:  0 | loss: 0.00023038101790007204\n",
            "Epoch:  173 | t:  0 | loss: 0.00612492207437753677\n",
            "Epoch:  174 | t:  0 | loss: 0.00011894576164195314\n",
            "Epoch:  175 | t:  0 | loss: 0.00022514857118949294\n",
            "Epoch:  176 | t:  0 | loss: 0.00018803993589244783\n",
            "Epoch:  177 | t:  0 | loss: 0.00006898561696289107\n",
            "Epoch:  178 | t:  0 | loss: 0.00008489267929689959\n",
            "Epoch:  179 | t:  0 | loss: 0.00011362088844180107\n",
            "Epoch:  180 | t:  0 | loss: 0.00016417073493357748\n",
            "Epoch:  181 | t:  0 | loss: 0.00007748380448902026\n",
            "Epoch:  182 | t:  0 | loss: 0.00009380919800605625\n",
            "Epoch:  183 | t:  0 | loss: 0.00063579611014574766\n",
            "Epoch:  184 | t:  0 | loss: 0.00002682399644982070\n",
            "Epoch:  185 | t:  0 | loss: 0.00011982359137618914\n",
            "Epoch:  186 | t:  0 | loss: 0.00047161200200207531\n",
            "Epoch:  187 | t:  0 | loss: 0.00028787789051420987\n",
            "Epoch:  188 | t:  0 | loss: 0.00002877607039408758\n",
            "Epoch:  189 | t:  0 | loss: 0.00022264255676418543\n",
            "Epoch:  190 | t:  0 | loss: 0.00004530083242570981\n",
            "Epoch:  191 | t:  0 | loss: 0.00101456663105636835\n",
            "Epoch:  192 | t:  0 | loss: 0.00004290869401302189\n",
            "Epoch:  193 | t:  0 | loss: 0.00004856066880165599\n",
            "Epoch:  194 | t:  0 | loss: 0.00137443025596439838\n",
            "Epoch:  195 | t:  0 | loss: 0.00192055688239634037\n",
            "Epoch:  196 | t:  0 | loss: 0.00030942109879106283\n",
            "Epoch:  197 | t:  0 | loss: 0.00005131842408445664\n",
            "Epoch:  198 | t:  0 | loss: 0.00008694457210367545\n",
            "Epoch:  199 | t:  0 | loss: 0.00009936790593201295\n",
            "Epoch:  200 | t:  0 | loss: 0.00053398148156702518\n",
            "Epoch:  201 | t:  0 | loss: 0.00543394032865762711\n",
            "Epoch:  202 | t:  0 | loss: 0.00171846267767250538\n",
            "Epoch:  203 | t:  0 | loss: 0.00027220894116908312\n",
            "Epoch:  204 | t:  0 | loss: 0.00067522452445700765\n",
            "Epoch:  205 | t:  0 | loss: 0.00012713944306597114\n",
            "Epoch:  206 | t:  0 | loss: 0.00004549861478153616\n",
            "Epoch:  207 | t:  0 | loss: 0.00085670821135863662\n",
            "Epoch:  208 | t:  0 | loss: 0.00095419702120125294\n",
            "Epoch:  209 | t:  0 | loss: 0.00034827773924916983\n",
            "Epoch:  210 | t:  0 | loss: 0.00009880544530460611\n",
            "Epoch:  211 | t:  0 | loss: 0.00005227445217315108\n",
            "Epoch:  212 | t:  0 | loss: 0.00167252635583281517\n",
            "Epoch:  213 | t:  0 | loss: 0.00016772991511970758\n",
            "Epoch:  214 | t:  0 | loss: 0.00337763573043048382\n",
            "Epoch:  215 | t:  0 | loss: 0.00064165593357756734\n",
            "Epoch:  216 | t:  0 | loss: 0.00030410560430027544\n",
            "Epoch:  217 | t:  0 | loss: 0.00003796497912844643\n",
            "Epoch:  218 | t:  0 | loss: 0.00057622778695076704\n",
            "Epoch:  219 | t:  0 | loss: 0.00036149282823316753\n",
            "Epoch:  220 | t:  0 | loss: 0.01303181890398263931\n",
            "Epoch:  221 | t:  0 | loss: 0.00137327087577432394\n",
            "Epoch:  222 | t:  0 | loss: 0.00010761489829747006\n",
            "Epoch:  223 | t:  0 | loss: 0.00014038461085874587\n",
            "Epoch:  224 | t:  0 | loss: 0.00094694853760302067\n",
            "Epoch:  225 | t:  0 | loss: 0.16970238089561462402\n",
            "Epoch:  226 | t:  0 | loss: 0.00427782814949750900\n",
            "Epoch:  227 | t:  0 | loss: 0.00169531954452395439\n",
            "Epoch:  228 | t:  0 | loss: 0.00073551572859287262\n",
            "Epoch:  229 | t:  0 | loss: 0.00103041867259889841\n",
            "Epoch:  230 | t:  0 | loss: 0.00014676059072371572\n",
            "Epoch:  231 | t:  0 | loss: 0.00010915932216448709\n",
            "Epoch:  232 | t:  0 | loss: 0.00068266986636444926\n",
            "Epoch:  233 | t:  0 | loss: 0.00011777579493355006\n",
            "Epoch:  234 | t:  0 | loss: 0.00013469644181896001\n",
            "Epoch:  235 | t:  0 | loss: 0.00009076915011974052\n",
            "Epoch:  236 | t:  0 | loss: 0.00002733676228672266\n",
            "Epoch:  237 | t:  0 | loss: 0.00024305909755639732\n",
            "Epoch:  238 | t:  0 | loss: 0.00013785986811853945\n",
            "Epoch:  239 | t:  0 | loss: 0.00009249518916476518\n",
            "Epoch:  240 | t:  0 | loss: 0.00028335818205960095\n",
            "Epoch:  241 | t:  0 | loss: 0.00013125392433721572\n",
            "Epoch:  242 | t:  0 | loss: 0.00003395112798898481\n",
            "Epoch:  243 | t:  0 | loss: 0.00048458465607836843\n",
            "Epoch:  244 | t:  0 | loss: 0.00014810441643930972\n",
            "Epoch:  245 | t:  0 | loss: 0.00036020940751768649\n",
            "Epoch:  246 | t:  0 | loss: 0.00103366957046091557\n",
            "Epoch:  247 | t:  0 | loss: 0.00032523047411814332\n",
            "Epoch:  248 | t:  0 | loss: 0.00016056268941611052\n",
            "Epoch:  249 | t:  0 | loss: 0.00016258606046903878\n",
            "Epoch:  250 | t:  0 | loss: 0.00178500125184655190\n",
            "Epoch:  251 | t:  0 | loss: 0.00008770047861617059\n",
            "Epoch:  252 | t:  0 | loss: 0.00012087525828974321\n",
            "Epoch:  253 | t:  0 | loss: 0.00006346407462842762\n",
            "Epoch:  254 | t:  0 | loss: 0.00102102477103471756\n",
            "Epoch:  255 | t:  0 | loss: 0.00006540978210978210\n",
            "Epoch:  256 | t:  0 | loss: 0.00002467511512804776\n",
            "Epoch:  257 | t:  0 | loss: 0.00166877894662320614\n",
            "Epoch:  258 | t:  0 | loss: 0.00031604233663529158\n",
            "Epoch:  259 | t:  0 | loss: 0.00010993909381795675\n",
            "Epoch:  260 | t:  0 | loss: 0.00003582655335776508\n",
            "Epoch:  261 | t:  0 | loss: 0.00008136456744978204\n",
            "Epoch:  262 | t:  0 | loss: 0.00030549115035682917\n",
            "Epoch:  263 | t:  0 | loss: 0.00013492914149537683\n",
            "Epoch:  264 | t:  0 | loss: 0.00001614186294318642\n",
            "Epoch:  265 | t:  0 | loss: 0.00206721713766455650\n",
            "Epoch:  266 | t:  0 | loss: 0.00028351860237307847\n",
            "Epoch:  267 | t:  0 | loss: 0.00071846065111458302\n",
            "Epoch:  268 | t:  0 | loss: 0.00026839430211111903\n",
            "Epoch:  269 | t:  0 | loss: 0.00007031948916846886\n",
            "Epoch:  270 | t:  0 | loss: 0.00013233539357315749\n",
            "Epoch:  271 | t:  0 | loss: 0.00015109777450561523\n",
            "Epoch:  272 | t:  0 | loss: 0.00008555682143196464\n",
            "Epoch:  273 | t:  0 | loss: 0.00021286211267579347\n",
            "Epoch:  274 | t:  0 | loss: 0.00014435927732847631\n",
            "Epoch:  275 | t:  0 | loss: 0.00015932622773107141\n",
            "Epoch:  276 | t:  0 | loss: 0.00346632581204175949\n",
            "Epoch:  277 | t:  0 | loss: 0.00071777275297790766\n",
            "Epoch:  278 | t:  0 | loss: 0.00084527028957381845\n",
            "Epoch:  279 | t:  0 | loss: 0.00038127638981677592\n",
            "Epoch:  280 | t:  0 | loss: 0.00029909668955951929\n",
            "Epoch:  281 | t:  0 | loss: 0.00016354005492758006\n",
            "Epoch:  282 | t:  0 | loss: 0.00021577718143817037\n",
            "Epoch:  283 | t:  0 | loss: 0.00006551257683895528\n",
            "Epoch:  284 | t:  0 | loss: 0.00028776054386980832\n",
            "Epoch:  285 | t:  0 | loss: 0.00496988557279109955\n",
            "Epoch:  286 | t:  0 | loss: 0.00892626214772462845\n",
            "Epoch:  287 | t:  0 | loss: 0.00412836950272321701\n",
            "Epoch:  288 | t:  0 | loss: 0.00069120171247050166\n",
            "Epoch:  289 | t:  0 | loss: 0.00495520839467644691\n",
            "Epoch:  290 | t:  0 | loss: 0.00012034140672767535\n",
            "Epoch:  291 | t:  0 | loss: 0.00020901282550767064\n",
            "Epoch:  292 | t:  0 | loss: 0.00045381340896710753\n",
            "Epoch:  293 | t:  0 | loss: 0.00165665056556463242\n",
            "Epoch:  294 | t:  0 | loss: 0.00408194912597537041\n",
            "Epoch:  295 | t:  0 | loss: 0.00013735187530983239\n",
            "Epoch:  296 | t:  0 | loss: 0.00333698210306465626\n",
            "Epoch:  297 | t:  0 | loss: 0.00011578173143789172\n",
            "Epoch:  298 | t:  0 | loss: 0.00092665210831910372\n",
            "Epoch:  299 | t:  0 | loss: 0.00046932042459957302\n",
            "Epoch:  300 | t:  0 | loss: 0.00005604901889455505\n",
            "Epoch:  301 | t:  0 | loss: 0.00004084760439582169\n",
            "Epoch:  302 | t:  0 | loss: 0.00022433757840190083\n",
            "Epoch:  303 | t:  0 | loss: 0.00007123142131604254\n",
            "Epoch:  304 | t:  0 | loss: 0.00006695691263303161\n",
            "Epoch:  305 | t:  0 | loss: 0.00015735930355731398\n",
            "Epoch:  306 | t:  0 | loss: 0.00031701833358965814\n",
            "Epoch:  307 | t:  0 | loss: 0.00006363320426316932\n",
            "Epoch:  308 | t:  0 | loss: 0.00007011250272626057\n",
            "Epoch:  309 | t:  0 | loss: 0.00079409143654629588\n",
            "Epoch:  310 | t:  0 | loss: 0.00007043316145427525\n",
            "Epoch:  311 | t:  0 | loss: 0.00021304941037669778\n",
            "Epoch:  312 | t:  0 | loss: 0.00013067075633443892\n",
            "Epoch:  313 | t:  0 | loss: 0.00013385310012381524\n",
            "Epoch:  314 | t:  0 | loss: 0.00211280304938554764\n",
            "Epoch:  315 | t:  0 | loss: 0.00063069997122511268\n",
            "Epoch:  316 | t:  0 | loss: 0.00022449083917308599\n",
            "Epoch:  317 | t:  0 | loss: 0.00032963298144750297\n",
            "Epoch:  318 | t:  0 | loss: 0.00445112353190779686\n",
            "Epoch:  319 | t:  0 | loss: 0.00009071610111277550\n",
            "Epoch:  320 | t:  0 | loss: 0.00061247195117175579\n",
            "Epoch:  321 | t:  0 | loss: 0.02072451636195182800\n",
            "Epoch:  322 | t:  0 | loss: 0.00013233136269263923\n",
            "Epoch:  323 | t:  0 | loss: 0.00203023734502494335\n",
            "Epoch:  324 | t:  0 | loss: 0.00009811805648496374\n",
            "Epoch:  325 | t:  0 | loss: 0.00625446811318397522\n",
            "Epoch:  326 | t:  0 | loss: 0.00281912600621581078\n",
            "Epoch:  327 | t:  0 | loss: 0.00058796757366508245\n",
            "Epoch:  328 | t:  0 | loss: 0.00116714264731854200\n",
            "Epoch:  329 | t:  0 | loss: 0.00014245326747186482\n",
            "Epoch:  330 | t:  0 | loss: 0.00038446893449872732\n",
            "Epoch:  331 | t:  0 | loss: 0.00017496169311925769\n",
            "Epoch:  332 | t:  0 | loss: 0.00012511019303929061\n",
            "Epoch:  333 | t:  0 | loss: 0.00007936180918477476\n",
            "Epoch:  334 | t:  0 | loss: 0.00035027804551646113\n",
            "Epoch:  335 | t:  0 | loss: 0.00003161323547828943\n",
            "Epoch:  336 | t:  0 | loss: 0.00036555758561007679\n",
            "Epoch:  337 | t:  0 | loss: 0.00002217089786427096\n",
            "Epoch:  338 | t:  0 | loss: 0.00080066284863278270\n",
            "Epoch:  339 | t:  0 | loss: 0.00010541863593971357\n",
            "Epoch:  340 | t:  0 | loss: 0.00153649819549173117\n",
            "Epoch:  341 | t:  0 | loss: 0.00296462746337056160\n",
            "Epoch:  342 | t:  0 | loss: 0.00008043131674639881\n",
            "Epoch:  343 | t:  0 | loss: 0.00085259974002838135\n",
            "Epoch:  344 | t:  0 | loss: 0.00041711528319865465\n",
            "Epoch:  345 | t:  0 | loss: 0.00087874988093972206\n",
            "Epoch:  346 | t:  0 | loss: 0.00003063363692490384\n",
            "Epoch:  347 | t:  0 | loss: 0.00114618230145424604\n",
            "Epoch:  348 | t:  0 | loss: 0.00063180789584293962\n",
            "Epoch:  349 | t:  0 | loss: 0.00014212624228093773\n",
            "Epoch:  350 | t:  0 | loss: 0.00018070047372020781\n",
            "Epoch:  351 | t:  0 | loss: 0.00025226236903108656\n",
            "Epoch:  352 | t:  0 | loss: 0.00297478795982897282\n",
            "Epoch:  353 | t:  0 | loss: 0.00074956240132451057\n",
            "Epoch:  354 | t:  0 | loss: 0.00011286915105301887\n",
            "Epoch:  355 | t:  0 | loss: 0.00016897852765396237\n",
            "Epoch:  356 | t:  0 | loss: 0.00007476059545297176\n",
            "Epoch:  357 | t:  0 | loss: 0.00034416961716488004\n",
            "Epoch:  358 | t:  0 | loss: 0.00114062172360718250\n",
            "Epoch:  359 | t:  0 | loss: 0.00007301717414520681\n",
            "Epoch:  360 | t:  0 | loss: 0.00054419622756540775\n",
            "Epoch:  361 | t:  0 | loss: 0.00012400622654240578\n",
            "Epoch:  362 | t:  0 | loss: 0.00005348888225853443\n",
            "Epoch:  363 | t:  0 | loss: 0.00013664264406543225\n",
            "Epoch:  364 | t:  0 | loss: 0.00311336223967373371\n",
            "Epoch:  365 | t:  0 | loss: 0.00149375107139348984\n",
            "Epoch:  366 | t:  0 | loss: 0.00058995751896873116\n",
            "Epoch:  367 | t:  0 | loss: 0.00145938037894666195\n",
            "Epoch:  368 | t:  0 | loss: 0.00007607581210322678\n",
            "Epoch:  369 | t:  0 | loss: 0.00238153780810534954\n",
            "Epoch:  370 | t:  0 | loss: 0.00640528975054621696\n",
            "Epoch:  371 | t:  0 | loss: 0.00210864446125924587\n",
            "Epoch:  372 | t:  0 | loss: 0.00003037923306692392\n",
            "Epoch:  373 | t:  0 | loss: 0.00051576696569100022\n",
            "Epoch:  374 | t:  0 | loss: 0.00259988429024815559\n",
            "Epoch:  375 | t:  0 | loss: 0.00017211053636856377\n",
            "Epoch:  376 | t:  0 | loss: 0.00035357719752937555\n",
            "Epoch:  377 | t:  0 | loss: 0.00073716975748538971\n",
            "Epoch:  378 | t:  0 | loss: 0.00054194696713238955\n",
            "Epoch:  379 | t:  0 | loss: 0.00513898069038987160\n",
            "Epoch:  380 | t:  0 | loss: 0.00010653316712705418\n",
            "Epoch:  381 | t:  0 | loss: 0.00028132542502135038\n",
            "Epoch:  382 | t:  0 | loss: 0.00047140524839051068\n",
            "Epoch:  383 | t:  0 | loss: 0.00012162147322669625\n",
            "Epoch:  384 | t:  0 | loss: 0.00067984865745529532\n",
            "Epoch:  385 | t:  0 | loss: 0.00042153458343818784\n",
            "Epoch:  386 | t:  0 | loss: 0.01120039727538824081\n",
            "Epoch:  387 | t:  0 | loss: 0.00027031797799281776\n",
            "Epoch:  388 | t:  0 | loss: 0.00015420508862007409\n",
            "Epoch:  389 | t:  0 | loss: 0.00049055332783609629\n",
            "Epoch:  390 | t:  0 | loss: 0.00007808586815372109\n",
            "Epoch:  391 | t:  0 | loss: 0.00016191680333577096\n",
            "Epoch:  392 | t:  0 | loss: 0.00020253924594726413\n",
            "Epoch:  393 | t:  0 | loss: 0.00007072355947457254\n",
            "Epoch:  394 | t:  0 | loss: 0.00003663096867967397\n",
            "Epoch:  395 | t:  0 | loss: 0.00015315997006837279\n",
            "Epoch:  396 | t:  0 | loss: 0.00020649802172556520\n",
            "Epoch:  397 | t:  0 | loss: 0.00019387406064197421\n",
            "Epoch:  398 | t:  0 | loss: 0.00022955803433433175\n",
            "Epoch:  399 | t:  0 | loss: 0.00046183599624782801\n",
            "Epoch:  400 | t:  0 | loss: 0.00011765777890104800\n",
            "Epoch:  401 | t:  0 | loss: 0.00008310665725730360\n",
            "Epoch:  402 | t:  0 | loss: 0.01263840403407812119\n",
            "Epoch:  403 | t:  0 | loss: 0.00007680184353375807\n",
            "Epoch:  404 | t:  0 | loss: 0.00010032128193415701\n",
            "Epoch:  405 | t:  0 | loss: 0.00005485385554493405\n",
            "Epoch:  406 | t:  0 | loss: 0.00031526613747701049\n",
            "Epoch:  407 | t:  0 | loss: 0.00036023079883307219\n",
            "Epoch:  408 | t:  0 | loss: 0.00207002111710608006\n",
            "Epoch:  409 | t:  0 | loss: 0.00055627012625336647\n",
            "Epoch:  410 | t:  0 | loss: 0.00113231339491903782\n",
            "Epoch:  411 | t:  0 | loss: 0.00081092619802802801\n",
            "Epoch:  412 | t:  0 | loss: 0.00374058960005640984\n",
            "Epoch:  413 | t:  0 | loss: 0.00020099556422792375\n",
            "Epoch:  414 | t:  0 | loss: 0.00022057928435970098\n",
            "Epoch:  415 | t:  0 | loss: 0.00007280203863047063\n",
            "Epoch:  416 | t:  0 | loss: 0.00047858711332082748\n",
            "Epoch:  417 | t:  0 | loss: 0.00200092140585184097\n",
            "Epoch:  418 | t:  0 | loss: 0.00042911243508569896\n",
            "Epoch:  419 | t:  0 | loss: 0.00017412014130968601\n",
            "Epoch:  420 | t:  0 | loss: 0.00006783295248169452\n",
            "Epoch:  421 | t:  0 | loss: 0.00030106364283710718\n",
            "Epoch:  422 | t:  0 | loss: 0.00341731961816549301\n",
            "Epoch:  423 | t:  0 | loss: 0.00035246449988335371\n",
            "Epoch:  424 | t:  0 | loss: 0.00173667119815945625\n",
            "Epoch:  425 | t:  0 | loss: 0.00226010568439960480\n",
            "Epoch:  426 | t:  0 | loss: 0.00034223112743347883\n",
            "Epoch:  427 | t:  0 | loss: 0.00044613162754103541\n",
            "Epoch:  428 | t:  0 | loss: 0.00190415279939770699\n",
            "Epoch:  429 | t:  0 | loss: 0.00023211685766000301\n",
            "Epoch:  430 | t:  0 | loss: 0.00014263471530284733\n",
            "Epoch:  431 | t:  0 | loss: 0.00051841506501659751\n",
            "Epoch:  432 | t:  0 | loss: 0.00543668167665600777\n",
            "Epoch:  433 | t:  0 | loss: 0.00030076887924224138\n",
            "Epoch:  434 | t:  0 | loss: 0.00040074795833788812\n",
            "Epoch:  435 | t:  0 | loss: 0.00024979174486361444\n",
            "Epoch:  436 | t:  0 | loss: 0.00021932256640866399\n",
            "Epoch:  437 | t:  0 | loss: 0.00014028207806404680\n",
            "Epoch:  438 | t:  0 | loss: 0.00095984176732599735\n",
            "Epoch:  439 | t:  0 | loss: 0.00018852538778446615\n",
            "Epoch:  440 | t:  0 | loss: 0.00056444091023877263\n",
            "Epoch:  441 | t:  0 | loss: 0.00117123650852590799\n",
            "Epoch:  442 | t:  0 | loss: 0.00074309704359620810\n",
            "Epoch:  443 | t:  0 | loss: 0.00006907132774358615\n",
            "Epoch:  444 | t:  0 | loss: 0.00020327120728325099\n",
            "Epoch:  445 | t:  0 | loss: 0.00015781637921463698\n",
            "Epoch:  446 | t:  0 | loss: 0.00348169868811964989\n",
            "Epoch:  447 | t:  0 | loss: 0.00013765823678113520\n",
            "Epoch:  448 | t:  0 | loss: 0.00041582089033909142\n",
            "Epoch:  449 | t:  0 | loss: 0.00078683375613763928\n",
            "Epoch:  450 | t:  0 | loss: 0.00028537702746689320\n",
            "Epoch:  451 | t:  0 | loss: 0.00167654734104871750\n",
            "Epoch:  452 | t:  0 | loss: 0.00020724549540318549\n",
            "Epoch:  453 | t:  0 | loss: 0.00052124919602647424\n",
            "Epoch:  454 | t:  0 | loss: 0.00017277263395953923\n",
            "Epoch:  455 | t:  0 | loss: 0.00023347989190369844\n",
            "Epoch:  456 | t:  0 | loss: 0.00030404573772102594\n",
            "Epoch:  457 | t:  0 | loss: 0.00213431380689144135\n",
            "Epoch:  458 | t:  0 | loss: 0.00114000309258699417\n",
            "Epoch:  459 | t:  0 | loss: 0.00031832742388360202\n",
            "Epoch:  460 | t:  0 | loss: 0.00013840409519616514\n",
            "Epoch:  461 | t:  0 | loss: 0.00016943580703809857\n",
            "Epoch:  462 | t:  0 | loss: 0.00055174605222418904\n",
            "Epoch:  463 | t:  0 | loss: 0.00014343604561872780\n",
            "Epoch:  464 | t:  0 | loss: 0.00027978693833574653\n",
            "Epoch:  465 | t:  0 | loss: 0.00055383169092237949\n",
            "Epoch:  466 | t:  0 | loss: 0.00035396486055105925\n",
            "Epoch:  467 | t:  0 | loss: 0.01661710441112518311\n",
            "Epoch:  468 | t:  0 | loss: 0.00021068569913040847\n",
            "Epoch:  469 | t:  0 | loss: 0.00017755597946234047\n",
            "Epoch:  470 | t:  0 | loss: 0.00054546730825677514\n"
          ]
        }
      ],
      "source": [
        "train(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JK_UGSVTaqI",
        "outputId": "43d02802-4de9-486a-e365-7de7dd1becf4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): LSTM(1, 64, num_layers=2, batch_first=True)\n",
              "  (decoder_cell): LSTMCell(1, 64)\n",
              "  (decoder_dense): Linear(in_features=64, out_features=1, bias=True)\n",
              "  (criterion): MSELoss()\n",
              ")"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = Seq2Seq(units=64, max_pred_len=100, start_token=0.00, end_token=0.000000000000000000000001).to(device)\n",
        "\n",
        "# Load the saved model state dictionary\n",
        "model.load_state_dict(torch.load('simple_model.pth'))\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmURz-yGZZ5P"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "first_feature = feature[0]\n",
        "first_real_lable = label_csv.iloc[0:1, 1:]\n",
        "\n",
        "# f1 = np.array(feature[0])\n",
        "# predict = np.array([model.inference(torch.tensor(np.array([f1[i * 100: (i + 1) * 100]]), dtype = torch.float32)).detach().numpy()[0] for i in range(int(f1.size / 100 - 1))])\n",
        "# predict = model.inference(torch.tensor(np.array([f1[497: 597]]), dtype = torch.float32)).detach().numpy()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "jxniclYXgPQ1",
        "outputId": "86a6c1a2-f4ca-408a-aea3-fd7743d6719c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "predict = model.inference(torch.tensor(np.array([np.array(feature.iloc[0:1, :])[0][497: 597]]), dtype = torch.float32)).detach().numpy()[0]\n",
        "plt.plot(predict, label='predict')\n",
        "plt.plot(first_real_lable.values[0][497:597], label='real')\n",
        "plt.xlabel('Time Step')\n",
        "plt.ylabel('Value')\n",
        "plt.title('Sample Sequences')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_Tlm9-ytXRk"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "xp-m0QjXr3-E",
        "outputId": "9cb1820a-9dc4-4efa-ed02-a69ee518c550"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "predict = np.array([model.inference(torch.tensor(np.array([np.array(feature.iloc[0:1, :])[0][i * 100: (i + 1) * 100]]), dtype = torch.float32)).detach().numpy()[0] for i in range(int(100))])\n",
        "plt.plot(np.concatenate((predict)), label='predict')\n",
        "plt.plot(first_real_lable.values[0], label='real')\n",
        "plt.xlabel('Time Step')\n",
        "plt.ylabel('Value')\n",
        "plt.title('Sample Sequences')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNq83SCYJPaSezhj+vMopPE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
